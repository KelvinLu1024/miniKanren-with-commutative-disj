\documentclass[format=acmlarge, review=true, authordraft=true]{acmart}

%% scheme-list :
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{trackchanges}

\addeditor{MVC}
\addeditor{LKC}
\addeditor{DPF}

\lstset{
  language=Scheme,
  basicstyle=\ttfamily,
  morekeywords={run,conde,run*,defrel,==,fresh,disj,conj,disj+},
  alsodigit={!\$\%&*+-./:<=>?@^_~},
  morecomment=[l]{;,\#{}lang},
  mathescape=true,
  literate={-inf}{{$^\infty$}}{1} {/fair}{{$_{fair}$}}{3}
  		   {→}{{$\rightarrow$}}{1}
  		   {×}{{$\times$}}{1}
  		   {==}{{$\equiv$}}{1}
  		   {conde}{{\textcolor[rgb]{0,.3,.7}{\textbf{cond$^e$}}}}{4}
  		   {repeato}{{repeat$^o$}}{6}
}
%% scheme-list .

% metadata

\title{Towards a miniKanren with fair search strategies}
\author{Kuang-Chen Lu}
\affiliation{Indiana University}
\author{Weixi Ma}
\affiliation{Indiana University}
\author{Daniel P. Friedman}
\affiliation{Indiana University}

\newcommand{\conde}{\texttt{cond$^e$}}
\newcommand{\conj}{\texttt{conj}}
\newcommand{\disj}{\texttt{disj}}
\newcommand{\clisting}[1]{
\begin{center}
  \begin{tabular}{c}
	\lstinputlisting{#1}
    \end{tabular}
\end{center}
}
\newcommand{\DFSi }[0]{DFS$_\textrm{i}$}
\newcommand{\DFSf }[0]{DFS$_\textrm{f}$}
\newcommand{\DFSbi}[0]{DFS$_\textrm{bi}$}
\newcommand{\BFS  }[0]{BFS}

\newcommand{\BFSopt}[0]{BFS$_\textrm{opt}$}

\newcommand{\BFSeff}[0]{BFS$_\textrm{eff}$}


\newcommand{\BFSser}[0]{BFS$_\textrm{ser}$}



\newcommand{\veryrecursiveo}[0]{\texttt{very-recursive$^o$}}
\newcommand{\appendo}[0]{\texttt{append$^o$}}
\newcommand{\reverso}[0]{\texttt{revers$^o$}}

\newtheorem{defn}{Definition}[section]

\begin{document}

\NOTE[LKC]{submission deadline: Mon 27 May 2019}

\begin{abstract}

  We describe fairness levels in disjunction and conjunction
  implementations.  Specifically, a disjunction implementation can be
  fair, almost-fair, or unfair. And a conjunction implementation can
  be fair or unfair.  We compare the fairness level of four search
  strategies: the standard miniKanren interleaving depth-first search (\DFSi),
  the balanced interleaving depth-first search (\DFSbi), the fair depth-first
  search (\DFSf), and the standard breadth-first search (\BFSser).  
  \DFSbi{} and \DFSf{} are new. And we present a new, more efficient
  and simpler implementation of \BFSser. Using quantitative evaluation, 
  we argue that
  \DFSbi{} and \DFSf{} are competitive alternatives to \DFSi, 
  and that \BFSopt{} is more efficient than the standard \BFSser.

% The syntax of a programming language should reflect its semantics. When writing 
% a \conde{} expression in miniKanren, a programmer would expect all clauses 
% share the same chance of being explored, as these clauses are written in 
% parallel. The existing search strategy, interleaving depth-first search 
% (\DFSi{}), 
% however, prioritizes its clauses by the order how they are written down. 
% Similarly, when a \conde{} is followed by another goal conjunctively, a 
% programmer would expect states in parallel share the same chance of being 
% explored. Again, the answers by \DFSi{} is different from the 
% expectation. We have devised three new search strategies that have different 
% level of fairness in \disj{} and \conj{}.
% 
% \NOTE[LKC]{\disj{} and \conj{} occurs free in the abstract.}

%The syntax of a programming language should reflect its semantics. When using 
%a disjunctive operator in relational programming, a programmer would expect 
%all 
%sub-goals of this disjunct to share the same chance of being explored, as 
%these 
%clauses are written in parallel. The existing multi-arity disjunctive operator 
%in miniKanren, however, prioritize its clauses by the order of which these 
%clauses are written down. We have devised two new search strategies that 
%allocate computational effort more fairly in all clauses.

\end{abstract}

\maketitle

\section{introduction}

miniKanren is a family of relational programming languages.
\citet{friedman_reasoned_2018} introduce miniKanren and its
implementation in \emph{The Reasoned Schemer, 2nd Ed}
(TRS2). miniKanren programs, have been proven to be useful in solving 
many problems as in miniKanren.org.\citet{byrd2017unified}.


% miniKanren is a family of relational programming languages.
% \citet{friedman_reasoned_2018} introduce miniKanren and its implementation
% in \emph{The Reasoned Schemer, 2nd Ed} (TRS2). miniKanren programs, especially
% relational interpreters, have been proven to be useful in solving many problems 
% as in \citet{byrd2017unified}.

\NOTE[DPF]{I think that the abbreviations for the search strategies
should not come up until the Introduction.}

\NOTE[DPF]{We should reference the earliest paper (See Will Byrd vita)}

\NOTE[DPF]{When we talk about complete and incomplete, I think we should
make it explicit that we are only talking about (run n (x y z) g ...) and
not (run* (x y z) g ...), but an alternative is that a stream is \emph{productive}
or \emph{unproductive}.}



\NOTE[LKC]{Shall we delete the sentence about relational interpreters? It is 
misleading: it is emphasizing relational interpreters, but we say nothing 
further about them later.}
\NOTE[DPF]{I have deleted the sentence.}
% \conde{} is the flagship disjunction operator in miniKanren.
A subtlety arises 
when a \conde{} contains many clauses: not every clause has an 
equal chance to contribute to the result. As an example, consider the following 
relation \texttt{repeato} and its invocation. 

\clisting{Figures/repeato.rkt}

Next, consider the following disjunction of invoking \texttt{repeato} with four 
different letters.

\clisting{Figures/example.rkt}

\conde{} intuitively relates its clauses with logical \texttt{or}. And thus an 
unsuspicious beginner would expect each letter to contribute equally to the 
result, as follows.

\clisting{Figures/run-repeato-fair.rkt}

The \conde{} in TR2, however, generates a less expected result.

\clisting{Figures/run-repeato-idfs.rkt}

The miniKanren in TRS2 implements interleaving DFS (\DFSi), the cause of this 
unexpected result. With this search strategy, each clause takes half 
of its received computational resources and passes the other half to its 
following clauses, except for the last clause that takes all resources it 
receives. In the example above, the \texttt{a} clause takes half of all 
resourses. And the \texttt{b} clause takes a quarter. Thus \texttt{c} and 
\texttt{d} barely contribute to the result.

%In the example above, the letters \texttt{c} and \texttt{d} are 
%allocated with less resources than \texttt{a}, and thus \texttt{c} and 
%\texttt{d} barely contribute to the result.

\DFSi{} is sometimes powerful for an expert. By carefully organizing the order 
of \conde{} clauses, a miniKanren program can explore more ``interesting'' 
clauses than those uninteresting ones, and thus use computational resources 
efficiently. A little miniKanrener, however, may beg to differ--understanding
implementation details and fiddling with clause order is not the first
priority of a beginner.

There is another reason that miniKanren could use more search strategies than
just \DFSi. In many applications, there does not exist one order that serves all purposes. For example, a relational dependent type checker contains
clauses for constructors that build data and clauses for eliminators that use
data. When the type checker is used to generate simple and shallow programs,
the clauses for constructors should be put at the top of the
\conde{} clauses.
When performing proof searches for complicated programs, the clauses for 
eliminators should be put at the top of \conde{} clauses. With \DFSi, these two uses cannot be 
efficient at the same time. In fact, to make one use efficient, the other one 
must be more sluggish.

The specification that every clause in the same \conde{} is given equal 
``search priority'' is called fair \disj{}. And search strategies with 
almost-fair \disj{} give every clause similar priority. 
Fair \conj{}, a related concept, is more subtle. The discussion of fair
\conj{} is covered in the next section.

To summarize our contribution, we
\begin{itemize}
	\item propose and implement balanced interleaving depth-first search 
	(\DFSbi{}), a new search strategy with almost-fair \disj{}.
	\item propose and implement fair depth-first search (\DFSf{}), a 
	new search strategy with fair \disj{}.
	\item implement in a new way breath-first search (\BFSopt), 
        a search strategy 
	with fair \disj{} and fair \conj{}. And we prove formally that our 
\BFSopt{} implementation is semantically equivalent to the one 
by \citet{seres1999algebra}, however, Our code 
runs faster in all benchmarks and we believe it is simpler.
\end{itemize}

\section{search strategies and fairness}

In this section, we define fair \disj{}, almost-fair \disj{} and fair \conj{}. 
Before going further into fairness, we give a short review of the terms:
\emph{state}, search \emph{space}, \emph{goal}.
A \emph{state} is a collection of constraints. (Here, we restrict 
constraints to unification constraints.) Every answer corresponds to a 
state. A \emph{space} is a collection of states. And a \emph{goal} is a 
function from a state to a space.

Now we elaborate fairness by running more queries about \texttt{repeato}.

\subsection{fair \texttt{disj}}

Given the following program, it is natural to expect lists of each letter to
constitute $1/4$ in the query result. \DFSi, the current search
strategy, however, results in many more lists of \texttt{a}s than lists
of other letters. And some letters  (e.g. \texttt{c} and \texttt{d}) are
rarely seen. The situation would be exacerbated if \conde{} would have
contained more clauses.

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-disj-DFSi.rkt}
	\end{tabular}
\end{center}

Under the hood, the \conde{} here is allocating computational resources to 
four trivially different search spaces. The unfair \disj{} in 
\DFSi{} allocates many more resources to the first search space. On the 
contrary, fair \disj{} would allocate resources evenly to each search space. 

\begin{center}
	\begin{tabular}{l|r}
		\lstinputlisting{Figures/repeato-disj-DFSf.rkt} &
		\lstinputlisting{Figures/repeato-disj-BFS.rkt}
	\end{tabular}
\end{center}

Running the same program again with almost-fair \disj {} (e.g. 
DFS$_\textrm{bi}$) gives the same result. Almost-fair, however, is not 
completely fair, as shown by the following example. 

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-disj-DFSbi.rkt}
	\end{tabular}
\end{center}

\DFSbi{} is fair only when the number of goals is a power of 2, 
otherwise, some goals would be allocated twice as many resources than the 
others. In the above example, where the \conde{} has five clauses, the clauses 
of \texttt{b}, \texttt{c}, and \texttt{d} are allocated more resources.

We end this subsection with precise definitions of all levels of 
\disj{} fairness. Our definition of \emph{fair} \disj{} is slightly 
more general
than the one in \citet{seres1999algebra}. Their definition is only
for binary disjunction. We generalize it to a multi-arity one.

\begin{defn}[fair \disj{}]
A \disj{} is fair if and only if it allocates computational resources evenly to 
search spaces produced by goals in the same disjunction 
(i.e., clauses in the same \conde).
\end{defn}

\begin{defn}[almost-fair \disj{}]
A \disj{} is almost-fair if and only if it allocates computational resources
evenly to search spaces produced by goals in the same disjunction that 
the maximal ratio of resources is bounded by a constant.
\end{defn}

\begin{defn}[unfair \disj{}]
A \disj{} is unfair if and only if it is not almost-fair.
\end{defn}

\subsection{fair \texttt{conj}}

\NOTE[LKC]{Checked grammer of text before this line}

Given the following program, it is natural to expect lists of each letter to
constitute $1/4$ in the answer. Search strategies with unfair \conj{} (e.g. 
\DFSi, \DFSbi, \DFSf), however, results in many more lists of \texttt{a}s than 
lists of other letters. And some letters are rarely seen. The situation would 
be exacerbated if \conde{} were to contain more clauses. Although 
DFS$_\textrm{i}$'s \disj{} is unfair in general, it is fair when there is no 
call to a relational definition in \conde{} clauses (including this case).

\begin{center}
\begin{tabular}{l|c|r}
    \lstinputlisting{Figures/repeato-conj-DFSi.rkt} &
    \lstinputlisting{Figures/repeato-conj-DFSf.rkt} &
    \lstinputlisting{Figures/repeato-conj-DFSbi.rkt} \\
\end{tabular}
\end{center}

Under the hood, the \conde{} and the call to \texttt{repeato} are connected by 
\conj{}. The \conde{} goal outputs a search space including four trivially 
different states. Then the next conjunctive goal, \texttt{(repeato x q)}, is 
applied to each of these states, producing four trivially different search 
spaces. In the examples above, the \conj{}s are allocating more computational 
resources to the search space of \texttt{a}. On the contrary, fair \conj{} 
would allocate resources evenly to each search space. For example,

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-conj-BFS.rkt}
	\end{tabular}
\end{center}

A more interesting situation is when the first conjunct produces an unbounded
number of states. Consider the following example, a naive specification of fair \conj{} 
might require search strategies to produce all sorts of singleton lists, but no 
longer ones, which makes the strategies incomplete. A search strategy is 
\emph{complete} if and only if it can determine all the answers within finite 
time, otherwise, it is \emph{incomplete}.

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-conj-infinite-naive.rkt}
	\end{tabular}
\end{center}

Our solution requires a search strategy with \emph{fair} \conj{} to organize
states in bags in search spaces, where each bag contains finite states, and 
to allocate resources evenly among search spaces derived from states in the 
same bag. It is up to a search strategy designer to decide by what criteria to 
put states in the same bag, and how to allocate resources among search spaces 
related to different bags.

\BFSser{} puts states of the same cost in the same bag, and allocates
resources carefully among search spaces related to different bags such
that answers are produced in increasing order of cost. The \emph{cost}
of an answer is its depth in the search tree (i.e., the number of
calls to relations required to find them) \citep{seres1999algebra}. In
the following example, every answer is a list of list of symbols,
where inner lists are the same. Here the cost of each answer is equal
to the length of its inner lists plus the length of its outer list.

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-conj-infinite-sof.rkt}
	\end{tabular}
\end{center}

We end this subsection with precise definitions of all levels of \conj{} 
fairness. 

% Our definition of fair \conj{} is orthogonal with completeness. For 
% example, a naively fair strategy is fair but not complete, while BFS is fair and 
% complete.

\begin{defn}[fair \conj{}]
A \conj{} is fair if and only if it allocates computational resources evenly to 
search spaces produced from states in the same bag. A bag is a finite 
collection of states. And search strategies with fair \conj{} should represent 
search spaces with possibly unbounded collections of bags. 
\end{defn}

\begin{defn}[unfair \conj{}]
A \conj{} is unfair if and only if it is not fair.
\end{defn}





\section{interleaving depth-first search}

In this section, we review the implementation of interleaving depth-first 
search (\DFSi). We focus on parts that are relevant to this paper. TRS2 
\citep{friedman_reasoned_2018} provides a comprehensive description of the 
whole miniKanren implementation.

\begin{figure}
	\lstinputlisting{Figures/conde.rkt}
	\caption{implementation of \conde}
	\label{conde}
\end{figure}

The definition of \conde{} (Fig.~\ref{conde}) is shared by all search 
strategies. It relates clauses disjunctively, and goals in the same 
clause conjunctively.

\begin{figure}
	\lstinputlisting{Figures/DFSi-0.rkt}
	\caption{implementation of \DFSi{} (Part I)}
	\label{DFSi-0}
\end{figure}

\begin{figure}
	\lstinputlisting{Figures/DFSi-1.rkt}
	\caption{implementation of \DFSi{} (Part II)}
	\label{DFSi-1}
\end{figure}

\NOTE[DPF]{If we use the name subscript, the subscripts should use an underscore and we should do it for gunderscorel and gundescorer. If we can use an l that
looks less like a 1, that would even be better.  Look at the l in TLT.}

Fig.~\ref{DFSi-0} and Fig.~\ref{DFSi-1} show parts that are later compared 
with other search 
strategies. We follow some conventions to name variables: variables bound to 
states are named \texttt{s}; variables bound to goals are named \texttt{g} 
(possibly with subscript); variables bound to search spaces have names ending 
with $^\infty$. Fig.~\ref{DFSi-0} shows the implementation of \disj. The 
first function, \texttt{disj2}, implements binary disjunction. It applies the 
two disjunctive goals to the input state \texttt{s} and composes the two 
resulting search spaces with \texttt{append$^\infty$}. The following syntax 
definitions say \disj{} is right-associative. Fig.~\ref{DFSi-1} 
shows the implementation of \conj. The first function, \texttt{conj2}, 
implements binary conjunction. 
It applies the \emph{first} goal to the input state, then applies the second 
goal to states in the resulting search space. The latter process is done with a 
the helper function \texttt{append-map$^\infty$} that applies its input goal to states 
in its input search spaces and composes the resulting search spaces. It reuses 
\texttt{append$^\infty$} for search space composition. The following syntax 
definitions say \conj{} is also right-associative.

\section{balanced interleaving depth-first search}

\begin{figure}
	\lstinputlisting{Figures/DFSbi.rkt}
	\caption{implementation of \DFSbi{}}
	\label{balanced-disj}
\end{figure}

Balanced interleaving DFS (DFS$_\textrm{bi}$) has an almost-fair \disj{} and unfair 
\conj{}. The implementation of DFS$_\textrm{bi}$ differs from 
DFS$_\textrm{i}$'s in the \disj{} macro. We list the new \disj{} with its 
helper in Fig.~\ref{balanced-disj}. When there are one or more disjunctive 
goals, \disj{} builds a balanced binary tree whose leaves are the goals and 
whose nodes are \texttt{disj2}s, hence the name of this search strategy. 
The new helper, \texttt{disj+}, takes two additional `arguments'. They 
accumulate goals to be put in the left and right subtrees. The first clause 
handles the case where there is only one goal. In this case, the tree is the 
goal itself. When there are more goals, we partition the list of goals 
into two sublists of roughly equal lengths and recur on the two sublists. Goals 
are moved to the accumulators in the last clause. As we are moving 
two goals each time, there are two base cases: (1) no goal remains; (2) one 
goal remains. These two new base cases are handled in the second clause and the 
third clause, respectively. In contrast, the \disj{} in \DFSi{} constructs the 
binary tree in a particularly unbalanced form.

\section{fair depth-first search}

\NOTE[DPF]{Why does the pair? case precede the null? case? Is it simply more 
efficient? It shouldn't be?}

\begin{figure}
	\lstinputlisting{Figures/DFSf.rkt}
	\caption{implementation of \DFSf{}}
	\label{fDFS}
\end{figure}

Fair DFS (\DFSf) has fair \disj{} and unfair \conj{}. The 
implementation of \DFSf{} differs from \DFSi{}'s in 
\texttt{disj2} (Fig.~\ref{fDFS}). \texttt{disj2} is changed to call a new and 
fair version of \texttt{append$^\infty$}. \texttt{append$^\infty_{fair}$} 
immediately 
calls 
its helper, \texttt{loop}, with the first argument, \texttt{s?}, set to 
\texttt{\#{}t}, which indicates that 
\texttt{s$^\infty$} and \texttt{t$^\infty$} haven't been swapped. The swapping 
happens at 
the third \texttt{cond} clause in the helper, where \texttt{s?} is updated 
accordingly. The first two \texttt{cond} clauses essentially copy the 
\texttt{car}s and stop recursion when one of the input spaces is obviously 
finite. The third clause, as we mentioned above, is just for swapping. When the 
fourth and last clause runs, we know that both \texttt{s$^\infty$} and 
\texttt{t$^\infty$} are ended with a thunk and they have been swapped. In this 
case, a new thunk is constructed. Here changing the order of 
\texttt{t$^\infty$} and \texttt{s$^\infty$} won't hurt the fairness (though it 
changes the order of answers). We swap them back so that answers are 
produced in a more natural order.

\NOTE[DPF]{When or where do you swap them back?}

\section{breadth-first search}

\BFSser{} has both fair \disj{} and fair \conj{}. Our implementation is based on 
DFS$_\textrm{f}$ (not DFS$_\textrm{i}$). To implement \BFSopt{} based on \DFSf{}, 
we need \texttt{append-map$^\infty_{fair}$} in addition to 
\texttt{append$^\infty_{fair}$}. 
The only difference between \texttt{append-map$^\infty$} and 
\texttt{append-map$^\infty_{fair}$} is that the latter calls 
\texttt{append$^\infty_{fair}$} instead of \texttt{append$^\infty$}.

The implementation can be improved in two ways. First, as mentioned in 
section 2.2, \BFSser puts answers in bags and answers of the same cost are in the 
same bag. In this implementation, however, it is unclear where this information 
is recorded. Second, \texttt{append$^\infty_{fair}$} is extravagant in memory 
usage. It 
makes $O(n+m)$ new \texttt{cons} cells every time, where $n$ and $m$ are the 
``length''s of input search spaces. We address these issues in the first 
subsection.

Both \BFSser{} \citet{seres1999algebra} and BFSopt{} produce answers in 
increasing order of cost. So it is interesting to see if they are equivalent. 
We prove so in Coq. The details are in the second subsection.

\subsection{optimized BFS}

\begin{figure}
	\lstinputlisting{Figures/BFS-opt.rkt}	
	\caption{new and changed functions in optimized BFS that implements pure 
	features}
	\label{BFS-opt}
\end{figure}

\NOTE[DPF]{Never refer to a literal section like 2.2. Instead
stick a label where it starts right near the section number and
stick another label where the subsection starts.
Then ref the two labels and place a period between them.}.

As mentioned in section 2.2, BFS puts answers in bags and answers of the 
same cost are in the same bag. The cost
information is recorded subtly -- the \texttt{car}s of a search space have cost 
0 (i.e. they are all in the same bag), and the costs of answers in thunk are 
computed recursively then increased by one. It is even more subtle that
\texttt{append$^\infty_{fair}$} and the \texttt{append-map$^\infty_{fair}$} 
respects the cost information. We make these facts more obvious by 
changing the type of search space, modifying related function definitions, 
and introducing a few more functions.

The new type is a pair whose \texttt{car} is a list of answers (the bag), and 
whose \texttt{cdr} is either a \texttt{\#{}f} or a thunk returning a search 
space. A falsy \texttt{cdr} means the search space is obviously finite. 

Functions related to the pure subset are listed in Fig.~\ref{BFS-opt} (the 
others in Fig.~\ref{BFS-opt-cont}). They are compared with 
\citeauthor{seres1999algebra}'s implementation in our proof. The first three 
functions in Fig.~\ref{BFS-opt} are search space constructors. \texttt{none} 
makes an empty search space; \texttt{unit} makes a space from one answer; and 
\texttt{step} makes a space from a thunk. The remaining functions as before.

Luckily, the change in \texttt{append$^\infty_{fair}$} also fixes the miserable 
space 
extravagance---the use of \texttt{append} helps us to reuse the first bag of 
\texttt{t$^\infty$}.

\citet{kiselyov2005backtracking} has shown that a \emph{MonadPlus} hides in 
implementations of logic programming system. The \BFSopt{} implementation is not 
an exception: \texttt{append-map$^\infty_{fair}$} is like \texttt{bind}, 
but takes arguments in reversed order; \texttt{none}, \texttt{unit}, and 
\texttt{append$^\infty_{fair}$} correspond to \texttt{mzero}, \texttt{unit}, 
and \texttt{mplus} respectively.

\begin{figure}
	\lstinputlisting{Figures/BFS-opt-cont.rkt}	
	\caption{new and changed functions in optimized BFS that implements impure 
		features}
	\label{BFS-opt-cont}
\end{figure}

Functions implementing impure features are in Fig.~\ref{BFS-opt-cont}. The 
first function, \texttt{elim}, takes a space \texttt{s$^\infty$} and two 
continuations \texttt{ks} and \texttt{kf}. When \texttt{s$^\infty$} contains 
some 
answers, \texttt{ks} is called with the first answer and the rest space. 
Otherwise, \texttt{kf} is called with no argument. Here `s' and `f' means 
`succeed' and `fail' respectively. This function is like an eliminator of 
search space, hence the name. The remaining functions do the same thing as 
before.

\subsection{comparison with the BFSser and BFSopt}

In this section, we compare the pure subset of \BFSopt{} with the BFS 
found in \citet{seres1999algebra}. We focus on the pure subset because 
their system is pure. Their system represents search spaces with streams of 
lists of answers, where each list is a bag.

To compare efficiency, we translate their Haskell code into Racket (See 
supplements for the translated code). The translation is direct 
due to the similarity in both logic programming systems and search space 
representations. The translated code is longer and slower than BFSopt
implementation. Details about 
difference in efficiency are in section 7.

We prove in Coq that \BFSser{} and \BFSopt{} are semantically equivalent, i.e. \texttt{(run n ? g)} 
produces the same result (See supplements for the formal proof).

\section{quantitative evaluation}

\begin{table}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		benchmark & size & DFS$_\textrm{i}$ & DFS$_\textrm{bi}$ & DFS$_\textrm{f}$ & BFS$_\textrm{opt}$ & BFS$_\textrm{ser}$  
		\\
		\hline
		\veryrecursiveo{} & 100000 &  579 &  793 & 2131 & 1438 & 3617 \\
		& 200000 & 1283 & 1610 & 3602 & 2803 & 4212 \\
		& 300000 & 2160 & 2836 &    - & 6137 &    - \\
		\hline 
		\appendo{}  & 100 &  31 &  41 &  42 &  31 &  68 \\ 
		& 200 & 224 & 222 & 221 & 226 & 218 \\ 
		& 300 & 617 & 634 & 593 & 631 & 622 \\ 
		\hline 
		\reverso{} & 10 &   5 &   3 &   3 &     38 &     85 \\ 
		& 20 & 107 &  98 &  51 &   4862 &   5844 \\
		& 30 & 446 & 442 & 485 & 123288 & 132159 \\ 
		\hline
		quine-1 & 1 &  71 &  44 & 69 & - & - \\ 
		& 2 & 127 & 142 & 95 & - & - \\ 
		& 3 & 114 & 114 & 93 & - & - \\ 
		\hline
		quine-2 & 1 & 147 & 112 &  56 & - & - \\ 
		& 2 & 161 & 123 & 101 & - & - \\ 
		& 3 & 289 & 189 & 104 & - & - \\ 
		\hline 
		'(I love you)-1 &  99 & 56 & 15 & 22 &  74 & 165 \\ 
		& 198 & 53 & 72 & 55 &  47 &  74 \\
		& 297 & 72 & 90 & 44 & 181 & 365 \\ 
		\hline
		'(I love you)-2 &  99 & 242 &  61 & 16 &  66 &  99 \\ 
		& 198 & 445 & 110 & 60 &  42 &  64 \\
		& 297 & 476 & 146 & 49 & 186 & 322 \\ 
		\hline 
	\end{tabular}
	\caption{The results of a quantitative evaluation: running times of 
	benchmarks 
		in milliseconds}
	\label{compare-efficiency}
\end{table}

Here we compare the efficiency of search strategies. A concise 
description is in Table~\ref{compare-efficiency}. A hyphen means ``running out of 
memory.'' The first two benchmarks are taken from 
TRS2. \reverso{} is from 
\citet{rozplokhas2018improving}. The next two benchmarks 
about quine are modified from a similar test case in \citet{byrd2017unified}. 
The modifications are made 
to circumvent the need for symbolic constraints (e.g. $\neq$, 
\texttt{absent$^o$}). Our version generates de 
Bruijnized expressions and prevent closures getting into list. The two 
benchmarks differ in the \conde{} clause order of their relational 
interpreters. 
The last two 
benchmarks are about synthesizing expressions that evaluate to \texttt{'(I love 
you)}. This benchmark is from \citet{byrd2017unified}. Again, the 
sibling benchmarks differ in the \conde{} clause order of their relational 
interpreters. The first one 
has elimination rules (i.e. application, \texttt{car}, and \texttt{cdr}) at the 
end, while the other has them at the beginning. We conjecture that DFS$_\textrm{i}$ would 
perform badly in the second case because elimination rules complicate the 
problem when synthesizing (i.e., our evaluation supports our conjecture.)

In general, only DFS$_\textrm{i}$ and DFS$_\textrm{bi}$ constantly perform well. DFS$_\textrm{f}$ is just as 
efficient in all benchmarks but \veryrecursiveo{}. Both BFS have obvious 
overhead in many cases. Among the three variants of DFS (they all have unfair 
\conj{}), DFS$_\textrm{f}$ is most resistant to clause permutation, followed by DFS$_\textrm{bi}$ then 
DFS$_\textrm{i}$. Among the two implementation of BFS, ours constantly performs as well or 
better. Interestingly, every strategy with fair \disj{} suffers in 
\veryrecursiveo{} and DFS$_\textrm{f}$ performs well elsewhere. 
Therefore, this 
benchmark might be a special case. Fair \conj{} imposes considerable overhead 
constantly except in \appendo{}. The reason might be that strategies with 
fair \conj{} tend to keep more intermediate answers in the memory.

\section{related works}

Yang \citet{yang2010adventures} points out a disjunct complex would be `fair' 
if it were a full and balanced tree .

Seres et al \citet{seres1999algebra} also describe a breadth-first search 
strategy. We prove \BFSser{} is semantically equivalent to \BFSopt. But our codeis shorter and performs better in comparison with a straightforward 
translation of their Haskell code.

\section{conclusion}

We analyze the definitions of fair \disj{} and fair \conj{}, then propose a 
new definition of fair \conj{}. Our definition is orthogonal with completeness.

We devise two new search strategies (i.e. balanced interleaving DFS 
(DFS$_\textrm{bi}$) and fair DFS (DFS$_\textrm{f}$)) and devise a new 
implementation of BFS. These strategies have different features 
in fairness: $_\textrm{bi}$ has an almost-fair \disj{} and unfair \conj{}. 
DFS$_\textrm{f}$ has fair \disj{} and unfair \conj{}. \BFSopt{} has both fair
\disj{} and fair \conj{}.

Our quantitative evaluation shows that DFS$_\textrm{bi}$ and DFS$_\textrm{f}$ are competitive 
alternatives to DFS$_\textrm{i}$, the current search strategy, and that 
\BFSopt is less practical than other search strategies.
\NOTE[DPF]{what}
\NOTE[LKC]{fixed}

We prove that \BFSopt{} is equivalent to \BFSser in \citet{seres1999algebra}. 
Our optimized code is shorter and runs faster than a direct translation 
of their Haskell code.

\section*{acknowledgments}

\bibliographystyle{ACM-Reference-Format}
\bibliography{citation}

\end{document}

