\documentclass[format=acmlarge, review=true, authordraft=true]{acmart}

%% scheme-list :
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}

\lstset{
  language=Scheme,
  basicstyle=\ttfamily,
  morekeywords={run,conde,run*,defrel,==,fresh},
  alsodigit={!\$\%&*+-./:<=>?@^_~},
  morecomment=[l]{;,\#{}lang},
  mathescape=true
}
%% scheme-list .

% metadata

\title{miniKanren with fair search strategies}
\author{Kuang-Chen Lu}
\affiliation{Indiana University}
\author{Weixi Ma}
\affiliation{Indiana University}
\author{Daniel P. Friedman}
\affiliation{Indiana University}



%%% NOTE %%%
%
%  SUBMISSION DEADLINE: May 15, 2019
%
%%%%%%%%%%%%


\newcommand{\conde}{\texttt{cond$^e$} }
\newcommand{\conj}{\texttt{conj}}
\newcommand{\disj}{\texttt{disj}}


\begin{document}

\begin{abstract}

The syntax of a programming language should reflect its semantics. When writing 
a \conde expression in miniKanren, a programmer would expect all clauses share 
the same chance of being explored, as these clauses are written in 
parallel. The existing search strategy, interleaving DFS (iDFS), however, 
prioritize its clauses by the order how they are written down. Similarly, when 
a \conde is followed by another goal conjunctively, a programmer would expect 
states in parallel share the same chance of being explored. Again, the answers 
by iDFS is different from the expectation. We have devised three new search 
strategies that have different level of fairness in \disj and \conj.

%The syntax of a programming language should reflect its semantics. When using 
%a disjunctive operator in relational programming, a programmer would expect 
%all 
%sub-goals of this disjunct to share the same chance of being explored, as 
%these 
%clauses are written in parallel. The existing multi-arity disjunctive operator 
%in miniKanren, however, prioritize its clauses by the order of which these 
%clauses are written down. We have devised two new search strategies that 
%allocate computational effort more fairly in all clauses.

\end{abstract}

\maketitle

\section{introduction}

miniKanren programs, especially relational interpreters, have been proven to be 
useful in solving many problems \citep{byrd2017unified}. A subtlety in writing 
relational programs having large \conde expressions, such as interpreters, is 
that the order of \conde clauses can affect the speed considerably. When a 
\conde expression is large enough, the left clauses consume almost all the 
resource and the right ones are hardly explored. The unfair \disj{} of the 
current search strategy, interleaving DFS, is the cause. Under the hood, \conde 
uses \texttt{conj} to create a goal for each clause, and \texttt{disj} to 
combine these goals to one. The current \texttt{disj} allocates half resource 
to its first goal, then allocates the other half to the rest similarly, except 
for the last clause which takes all the resource. 

Being aware of \texttt{disj} fairness, we also investigate \texttt{conj} 
fairness. 

We propose three new search strategies, balanced interleaving DFS (biDFS), fair 
DFS (fDFS), and BFS. They have different characteristics of fairness 
(Table.~\ref{fairness}). We prove that the pure subset of our BFS and the BFS 
proposed by Seres et
al~\citep{seres1999algebra} produce the same result when queried. But our code 
is shorter and runs faster. Because the two BFSs are equivalent, we do not 
distinguish them in most places.
 
\begin{table}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		fairness & iDFS & biDFS & fDFS & BFS \\ 
		\hline 
		disj & unfair & almost-fair & fair & fair \\ 
		\hline 
		conj & unfair & unfair & unfair & fair \\ 
		\hline 
	\end{tabular} 
	\caption{fairness of search strategies}
	\label{fairness}
\end{table}

% Interleaving DFS, the search strategy of \conde in
% miniKanren~\citep{friedman_reasoned_2018}, allocates computational resource
% by the order in which the clauses are written. Each clause of a
% \conde takes half of the current resource and passes the other half
% to its following clauses, except for the last clause that takes all of the 
%current resource. 
% The biased treatment provides both opportunity and burden: miniKanren users 
%can place more frequently used 
% goal at the beginning to optimize their programs; however, it might be a 
% catastrophe if a goal that generates many useless states is placed before 
%more 
% important goals. Seasoned miniKanreners usually know how to utilize 
% the unfairness to optimize their programs. However, we believe search 
%strategies that is less sensitive to goal order can also be useful to little 
%miniKanreners as well as seasoned ones. We propose two such search strategies, 
%balanced interleaving DFS (biDFS) and breadth-first search (BFS), and observe 
%how they affect the efficiency and the answer order of known miniKanren 
%programs. The experiment is conducted with the miniKanren from \textit{The 
%Reasoned Schemer, 2nd Edition}.

\section{fairness}

We demonstrate the aspects (\disj{} or \conj{}) and levels (unfair, 
almost-fair, or fair) of fairness by running queries about \texttt{repeato}, a 
relational definition that relates a term \texttt{x} with a non-empty list 
whose elements are \texttt{x} (Fig.~\ref{repeato}).

\begin{figure}
	\lstinputlisting{Figures/repeato.rkt}
	\caption{\texttt{repeato} and an example run}
	\label{repeato}
\end{figure}

\subsection{fair \texttt{disj}}

In the following program, the three \conde clauses differ in a trivial way. So 
we expect lists of each sort constitute $1/4$ of the answer list. However, 
iDFS, the current search strategy, gives us much more lists of \texttt{a} than 
other sorts of lists. And some sorts (e.g. lists of \texttt{c}) are hardly 
found. The situation would be worse if we add more \conde clauses.

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-disj-iDFS.rkt}
	\end{tabular}
\end{center}

We borrow the definition of \emph{fair} \disj{} from \cite{seres1999algebra}: 
search strategies with \emph{fair} \disj{} should allocate resource evenly 
among disjunctive goals. Running the same program with fDFS and BFS give the 
following result. 

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-disj-fair.rkt}
	\end{tabular}
\end{center}

We conceive a middle place between fair and unfair -- search strategies with 
\emph{almost-fair} \disj{} should allocate resource so evenly among disjunctive 
goals\citep{seres1999algebra} that the the maximal ratio of resource is bounded 
by a constant. Our new search strategy, biDFS, has almost-fair \disj{}. It is 
fair when the number of goals is a power of 2, otherwise, some goals are 
allocated twice more resource than the others. In the previous example, it 
gives the same result. And in the following example, where the \conde has 5 
clause, the clauses of \texttt{b}, \texttt{c}, and \texttt{d} are allocated 
more resource.

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-disj-biDFS.rkt}
	\end{tabular}
\end{center}

\subsection{fair \texttt{conj}}

In the following program, the three \conde clauses differ in a trivial way. So 
we expect lists of each sort constitute $1/4$ of the answer list. However, 
search strategies with unfair \conj{} (e.g. iDFS, fDFS) give us much 
more lists of \texttt{a} than other sorts of lists. And some sorts (e.g. lists 
of \texttt{c}) are hardly found. Although iDFS's \disj{} is unfair in general, 
it is fair when there is no call to relational definition in sub-goals, 
including this case. The situation would be worse if we add more \conde 
clauses. The result with biDFS, whose \conj{} is also unfair, is similar, but 
due to its different \disj{}, the position of \texttt{b} and \texttt{c} are 
swapped. 

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-conj-unfair.rkt}
	\end{tabular}
\end{center}

Intuitively, search strategies with fair \conj{} should produce each sort of 
lists equally frequently. Indeed, BFS does so.

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-conj-fair.rkt}
	\end{tabular}
\end{center}

A more interesting situation is when the first conjunctive goal produces 
infinite many states. Consider the following example, a naive specification of 
fair \conj{} might require search strategies to produce all sorts of singleton 
lists, but no longer ones, which makes the strategies \emph{incomplete}. 

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-conj-infinite-naive.rkt}
	\end{tabular}
\end{center}

Our solution is requiring a search strategy with \emph{fair} \conj{} to 
package states in bags, where each bag contains finite states, and to allocate 
resource evenly among search spaces derived from states in the same bag. The 
way to package depends on search strategy. And how to allocate resource among 
search space related to different bags is unspecified. Our definition of fair 
\conj{} is orthogonal with completeness. For example, a naively fair strategy 
is fair but not complete, while BFS is fair and complete. 

BFS packages states by their costs. The \emph{cost} of a state is its 
depth in the search tree (i.e. the number of calls to relational definitions 
required to find them) \citep{seres1999algebra}. In the following example, the 
cost of an answer is equal to the length of the inner lists plus the length of 
the outer list. Noted that BFS produces answers in increasing order of cost.

\begin{center}
	\begin{tabular}{c}
		\lstinputlisting{Figures/repeato-conj-infinite-sof.rkt}
	\end{tabular}
\end{center}

\section{balanced interleaving depth-first search}

\begin{figure}
	\lstinputlisting{Figures/balanced-disj.rkt}
	\caption{\texttt{balanced-disj}}
	\label{balanced-disj}
\end{figure}

Balanced interleaving DFS (biDFS) has almost-fair \disj{} and unfair \conj{}. 
The implementation of biDFS differs from iDFS in the \disj{} macro. We list the 
new \disj{} with its helpers in Fig.~\ref{balanced-disj}. The first helper 
function, \texttt{split}, takes a list of goals \texttt{ls} and a procedure 
\texttt{k}, partitions \texttt{ls} into two sub-lists of roughly equal length, 
and returns the application of \texttt{k} to the two sub-lists. \texttt{disj*} 
takes a non-empty list of goals \texttt{gs} and returns a goal. With the help 
of \texttt{split}, it essentially constructs a \emph{balanced} binary tree 
where leaves are elements of \texttt{gs} and nodes are \texttt{disj2}, whence 
the name of this search strategy. In contrast, the \disj{} in iDFS essentially 
constructs the same sort of binary tree in one of the most unbalanced forms.

\section{fair depth-first search}

\begin{figure}
	\lstinputlisting{Figures/fDFS.rkt}
	\caption{How fDFS differs from iDFS}
	\label{fDFS}
\end{figure}

Fair DFS (fDFS) has fair \disj{} and unfair \conj{}. The implementation of fDFS 
differs from iDFS's in \texttt{disj2} (Fig.~\ref{fDFS}). \texttt{disj2} is 
changed to call a new and fair version of \texttt{append-inf}. 
\texttt{append-inf/fair} immediately calls its helper, 
\texttt{append-inf/fair\^{}}, with the first argument, \texttt{s?}, set to 
\texttt{\#{}t}, which indicates that \texttt{s-inf} and \texttt{t-inf} haven't 
been swapped. The swapping happens at the third \texttt{cond} clause in the 
helper, where \texttt{s?} is updated accordingly. The first two \texttt{cond} 
clauses essentially copy the \texttt{car}s and stop recursion when one of the 
input spaces is obviously finite. The third clause, as we mentioned early, is 
just for swapping. When the fourth and last clause runs, we know that both 
\texttt{s-inf} and \texttt{t-inf} are ended with a thunk. In this case, a new 
thunk is constructed. The new thunk calls the driver recursively. Here changing 
the order of \texttt{t-inf} and \texttt{s-inf} won't hurt the fairness (though 
it will change the order of answers). We swapped them back so that answers are 
produced in a more natural order.


\section{breadth-first search}

BFS is fair in both \disj{} and \conj{}. Our implementation is based on 
fDFS (not iDFS). All we have to do is apply two trivial changes to 
\texttt{append-map-inf}. First, rename it to \texttt{append-map-inf/fair}. 
Second, replace its use of \texttt{append-inf} to \texttt{append-inf/fair}. 

The implementation can be improved in two aspects. First, as we mentioned in 
section 2.2, our BFS bag states by their cost. However, in this implementation, 
it is unclear where this information is recorded. Second, 
\texttt{append-inf/fair} is extravagant in memory usage. It makes 
$O(n+m)$ new \texttt{cons} cells every time, where $n$ and $m$ are the 
``length''s of input search spaces. We address these issues in the first 
subsection.

Both our BFS and Seres's BFS \citep{seres1999algebra} produce answers in 
increasing order of cost. So it is interesting to see if they are equivalent. 
We prove so in Coq. The details are in the second subsection.

\subsection{optimized BFS}

\begin{figure}
	\lstinputlisting{Figures/BFS-opt.rkt}	
	\caption{interface functions in optimized BFS (pure)}
	\label{BFS-opt}
\end{figure}

As we mentioned in section 2.2, our BFS bag states by their cost. The bagging 
information is recorded subtly -- the \texttt{car}s of a search space have cost 
0 (i.e. they are in the same bag), and the costs of states in thunk are 
computed recursively then increased by one. It is even more subtle that
\texttt{append-inf/fair} and the \texttt{append-map-inf/fair} respects the cost 
information. We make these facts more obvious by changing the type of search 
space, modifying related function definitions, and introducing a few more 
functions.

The new type is a pair whose \texttt{car} is a list of state (the bag), and 
whose \texttt{cdr} is either a \texttt{\#{}f} or a thunk returning a search 
space. A falsy \texttt{cdr} means the search space is obviously finite. 

Functions related to the pure subset are listed in Fig.~\ref{BFS-opt} (the 
others in Fig.~\ref{BFS-opt-cont}). We take them apart because they are 
compared with Seres's implementation later. The first three functions in 
Fig.~\ref{BFS-opt} are search space constructor. \texttt{none} makes an empty 
search space. \texttt{unit} makes an space from one state. \texttt{step} makes 
a space from a thunk. The remaining functions do the same thing as before. 

Luckily, the change in \texttt{append-inf/fair} also fixes the miserable space 
extravagance -- the use of \texttt{append} helps us to reuse the first bag of 
\texttt{t-inf}.

Noted that some functions here constitute a \emph{MonadPlus}: 
\texttt{none}, \texttt{unit}, \texttt{append-map-inf}, and \texttt{append-inf} 
correspond to \texttt{mzero}, \texttt{unit}, \texttt{bind}, and \texttt{mplus} 
respectively.

\begin{figure}
	\lstinputlisting{Figures/BFS-opt-cont.rkt}	
	\caption{interface functions in optimized BFS (impure)}
	\label{BFS-opt-cont}
\end{figure}

Functions implementing impure features are in Fig.~\ref{BFS-opt-cont}. The 
first function, \texttt{elim}, takes a space \texttt{s-inf} and two 
continuations \texttt{ks} and \texttt{kf}. When \texttt{s-inf} contains some 
states, \texttt{ks} is called with the first state and the rest space. 
Otherwise, \texttt{kf} is called with no argument. Here `s' and `f' means 
`succeed' and `fail' respectively. This function is an eliminator of search 
space, whence the name. The remaining functions do the same thing as before.

\subsection{comparison with Silvija's BFS}

In this section, we compare the pure subset of our optimized BFS with the BFS 
found in \citep{seres1999algebra}. We focus on the pure subset because 
Silvija's system is pure. 

To compare efficiency, we translate her Haskell code into Racket (See 
supplements for the translated code). The translation is fairly straightforward 
due to the similarity in both logic programming system and search space 
representation. The translated code is longer and slower. Details about 
efficiency difference are in section 6.

We prove the two search strategies are equivalent in Coq. Since search space 
can be infinite, we should use a co-inductive data type. However, Coq is too 
strict in the guardedness condition to accept a direct translation of the 
implementations. Therefore, we prove core theorems with finite search space 
instead. In order to generalize the conclusion to the cases with infinite 
search space, we prove a few more theorems saying that whenever we query 
answers lower than some finite cost, we can restrict goals to truncate search 
spaces at some finite depth without changing the query result. (See supplements 
for the formal proof)

\section{quantitative evaluation}

;;TODO check if reverso and appendo are absolutely the same as the ones in TRS2.

\begin{table}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		benchmark & size & iDFS & biDFS & fDFS & optimized BFS & Silvija's BFS  
		\\
		\hline
		very-recursiveo & 100000 &  579 &  793 & 2131 & 1438 & 3617 \\
		& 200000 & 1283 & 1610 & 3602 & 2803 & 4212 \\
		& 300000 & 2160 & 2836 &    - & 6137 &    - \\
		\hline 
		appendo  & 100 &  31 &  41 &  42 &  31 &  68 \\ 
		& 200 & 224 & 222 & 221 & 226 & 218 \\ 
		& 300 & 617 & 634 & 593 & 631 & 622 \\ 
		\hline 
		reverseo & 10 &   5 &   3 &   3 &     38 &     85 \\ 
		& 20 & 107 &  98 &  51 &   4862 &   5844 \\
		& 30 & 446 & 442 & 485 & 123288 & 132159 \\ 
		\hline
		quine-1 & 1 &  71 &  44 & 69 & - & - \\ 
		& 2 & 127 & 142 & 95 & - & - \\ 
		& 3 & 114 & 114 & 93 & - & - \\ 
		\hline
		quine-2 & 1 & 147 & 112 &  56 & - & - \\ 
		& 2 & 161 & 123 & 101 & - & - \\ 
		& 3 & 289 & 189 & 104 & - & - \\ 
		\hline 
		'(I love you)-1 &  99 & 56 & 15 & 22 &  74 & 165 \\ 
		& 198 & 53 & 72 & 55 &  47 &  74 \\
		& 297 & 72 & 90 & 44 & 181 & 365 \\ 
		\hline
		'(I love you)-2 &  99 & 242 &  61 & 16 &  66 &  99 \\ 
		& 198 & 445 & 110 & 60 &  42 &  64 \\
		& 297 & 476 & 146 & 49 & 186 & 322 \\ 
		\hline 
	\end{tabular}
	\caption{The results of a quantitative evaluation: running times of 
	benchmarks 
		in milliseconds}
	\label{compare-efficiency}
\end{table}

In this section, we compare the efficiency of search strategies. A concise 
description is in Table~\ref{compare-efficiency}. A hyphen means running out of 
memory. The first three benchmarks are taken from 
\citep{friedman_reasoned_2018}. Next two benchmarks about quine are modified 
from a similar test case in \citep{byrd2017unified}. The modifications are made 
to circumvent the need for symbolic constraints (e.g. $\neq$, 
\texttt{absento}). Our version generates de 
Bruijnized expressions and prevent closures getting into list. The two 
benchmarks differ in the \conde clause order of their relation interpreters. 
The last two 
benchmarks are about synthesizing expressions that evaluate to \texttt{'(I love 
you)}. This benchmark is also inspired by \citep{byrd2017unified}. Again, the 
sibling benchmarks differ in the \conde clause order of their relation 
interpreters. The first one 
has elimination rules (i.e. application, \texttt{car}, and \texttt{cdr}) at the 
end, while the other has them at the beginning. We conjecture that iDFS would 
perform badly in the second case because elimination rules complicate the 
problem when running backward. The evaluation supports our conjecture.

In general, only iDFS and biDFS constantly perform well. fDFS is just as 
efficient in all benchmarks but \texttt{very-recursiveo}. Both BFS have obvious 
overhead in many cases. Among the three variants of DFS (they all have unfair 
\conj{}), fDFS is most resistant to clause permutation, followd by biDFS then 
iDFS. Among the two implementation of BFS, ours constantly performs as well or 
better. Interestingly, every strategies with fair \disj{} suffers in 
\texttt{very-recursiveo} and fDFS performs well elsewhere. Therefore, this 
benchmark might be a special case. Fair \conj{} imposes overhead constantly 
except in \texttt{appendo}. The reason might be that strategies with fair 
\conj{} tend to keep more intermediate states in the memory.

\section{related works}

Edward points out a disjunct complex would be `fair' if it is a full and 
balanced tree \citep{yang2010adventures}.

Silvija et al \citep{seres1999algebra} also describe a breadth-first search 
strategy. We proof their BFS is equivalent to ours. However, ours looks simpler 
and performs better in comparison with a straightforward translation of their 
Haskell code.

\section{conclusion}

We analysis the definitions of fair \disj{} and fair \conj{}, then propose a 
new definition of fair \conj{}. Our definition is orthogonal with completeness.

We devise three new search strategies: balanced interleaving DFS (biDFS), fair 
DFS (fDFS), and BFS. biDFS has almost-fair \disj{} and unfair \conj{}. fDFS has 
fair \disj{} and unfair \conj{}. BFS has both fair \disj{} and fair \conj{}.

Our quantitative evaluation shows that biDFS and fDFS are competitive 
alternatives to iDFS, the current search strategy, and that BFS is less 
practical.

We prove our BFS is equivalent to the BFS in \citep{seres1999algebra}. Our code 
is shorter and runs faster than a direct translation of their Haskell code.

\section*{acknowledgments}

\bibliographystyle{ACM-Reference-Format}
\bibliography{citation}

\end{document}

